{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "In this tutorial, we'll see how we can train a language model using the built-in datasets in torchtext.\n",
    "\n",
    "We'll also take a look at some more practical features of torchtext that you might want to use when training your own practical models.\n",
    "This tutorial assumes that you have access to a GPU for the sake of training speed. If you don't have a GPU, you can change the following variable `USE_GPU` to False. Be warned though, since the training will be very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is Language Modeling?\n",
    "Language modeling is a task where we build a model that can take a sequence of words as input and determine how likely that sequence is to be actual human language. For instance, we would want our model to predict \"This is a sentence\" to be a likely sequence and \"cold his book her\" to be unlikely.\n",
    "\n",
    "The way we generally train language models is by training them to predict the next word given all previous words in a sentence or multiple sentences. Therefore, all we need to do language modeling is a large amount of language data (called a corpus).\n",
    "\n",
    "In this tutorial, we'll be using the famous WikiText2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last tutorial we tokenized on spaces. This time, we'll use a slightly more sophisticated tokenizer: the spacy tokenizer.\n",
    "\n",
    "[Spacy](https://spacy.io/) is a framework that handles many natural language processing tasks, and torchtext is designed to work closely with it.\n",
    "\n",
    "Using the tokenizer is easy with torchtext: all we have to do is pass in the tokenizer function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.symbols import ORTH\n",
    "my_tok = spacy.load('en')\n",
    "my_tok.tokenizer.add_special_case('<eos>', [{ORTH: '<eos>'}])\n",
    "my_tok.tokenizer.add_special_case('<bos>', [{ORTH: '<bos>'}])\n",
    "my_tok.tokenizer.add_special_case('<unk>', [{ORTH: '<unk>'}])\n",
    "def spacy_tok(x):\n",
    "    return [tok.text for tok in my_tok.tokenizer(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`add_special_case` simply tells the tokenizer to parse a certain string in a certain way. The list after the special case string represents how we want the string to be tokenized. \n",
    "\n",
    "If we wanted to tokenize \"don't\" into \"do\" and \"'nt\", then we would write\n",
    "\n",
    "`my_tok.tokenizer.add_special_case(\"don't\", [{ORTH: \"do\"}, {ORTH: \"n't\"}])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to initialize the text field by ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=spacy_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll load the built-in datasets.\n",
    "There are two effective ways of using these datasets: one is loading as a Dataset split into the train, validation, and test sets, and the other is loading as an Iterator. The dataset offers more flexibility, so we'll use that approach here.\n",
    "\n",
    "There is currently one built-in dataset for language modeling: the WikiText2 dataset. (I've sent a pull request for the also commonly used and slightly smaller dataset called the Penn Treebank dataset. If you install the version on [my fork](https://github.com/keitakurita/text@penn_treebank), you can use it in place and have the code run faster!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = WikiText2.splits(TEXT) # loading custom datasets requires passing in the field, but nothing else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look inside. Remember, datasets behave largely like normal lists, so we can measure the length using the `len` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one training example?! Did we do something wrong?\n",
    "\n",
    "Turns out not. It's just that the entire corpus of the dataset is contained within a single example. We'll see how this example gets batched and processed later.\n",
    "\n",
    "Now that we have our data, let's build the vocabulary. This time, let's try using precomputed word embeddings.\n",
    "\n",
    "We'll use GloVe vectors with 200 dimensions this time. There are various other precomputed word embeddings in torchtext (including GloVe vectors with 100 and 300 dimensions) as well which can be loaded in mostly the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors=\"glove.6B.200d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build our iterator. This is the climax of this tutorial!\n",
    "It turns out that torchtext has a very handy iterator that does most of the heavy lifting for us. It's called the `BPTTIterator`.\n",
    "The `BPTTIterator` does the following for us:\n",
    "- Divide the corpus into batches of sequence length `bptt`\n",
    "\n",
    "For instance, suppose we have the following corpus: \n",
    "\n",
    "*\"Machine learning is a field of computer science that gives computers the ability to learn without being explicitly programmed.\"*\n",
    "\n",
    "Though this sentence is short, the actual corpus is thousands of words long, so we can't possibly feed it in all at once. We'll want to divide the corpus into sequences of a shorter length. In the above example, if we wanted to divide the corpus into batches of sequence length 5, we would get the following sequences:\n",
    "\n",
    "[\"*Machine*\", \"*learning*\", \"*is*\", \"*a*\", \"*field*\"],\n",
    "\n",
    "[\"*of*\", \"*computer*\", \"*science*\", \"*that*\", \"*gives*\"],\n",
    "\n",
    "[\"*computers*\", \"*the*\", \"*ability*\", \"*to*\", \"*learn*\"],\n",
    "\n",
    "[\"*without*\", \"*being*\", \"*explicitly*\", \"*programmed*\", EOS]\n",
    "\n",
    "\n",
    "- Generate batches that are the input sequences offset by one\n",
    "\n",
    "In language modeling, the supervision data is the next word in a sequence of words. We, therefore, want to generate the sequences that are the input sequences offset by one. In the above example, we would get the following sequence that we train the model to predict:\n",
    "\n",
    "[\"*learning*\", \"*is*\", \"*a*\", \"*field*\", \"*of*\"],\n",
    "\n",
    "[\"*computer*\", \"*science*\", \"*that*\", \"*gives*\", \"*computers*\"],\n",
    "\n",
    "[\"*the*\", \"*ability*\", \"*to*\", \"*learn*\", \"*without*\"],\n",
    "\n",
    "[\"*being*\", \"*explicitly*\", \"*programmed*\", EOS, EOS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    bptt_len=30, # this is where we specify the sequence length\n",
    "    device=(0 if USE_GPU else -1),\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, it's a good idea to take a look into what is actually happening behind the scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'train', 'text', 'target'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(b).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We never specified a target field, so it must have been automatically generated. Hopefully, it's the original text offset by one. Let's see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "     9    953      0\n",
       "    10    324   5909\n",
       "     9     11  20014\n",
       "    12   5906     27\n",
       "  3872  10434      2\n",
       "  3892      3  10780\n",
       "   886     11   3273\n",
       "    12   9357      0\n",
       "    10   8826  23499\n",
       "     9   1228      4\n",
       "    10      7    569\n",
       "     9      2    235\n",
       " 20059   2592   5909\n",
       "    90      3     20\n",
       "  3872    141      2\n",
       "    95      8   1450\n",
       "    49   6794    369\n",
       "     0   9046      5\n",
       "  3892   1497      2\n",
       "    24     13   2168\n",
       "   786      4    488\n",
       "    49     26   5967\n",
       " 28867     25    656\n",
       "     3  18430     14\n",
       "  6213     58     48\n",
       "     4   4886   4364\n",
       "  3872    217      4\n",
       "     5      5     22\n",
       "     2      2   1936\n",
       "  5050    593     59\n",
       "[torch.cuda.LongTensor of size 30x3 (GPU 0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.text[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    10    324   5909\n",
       "     9     11  20014\n",
       "    12   5906     27\n",
       "  3872  10434      2\n",
       "  3892      3  10780\n",
       "   886     11   3273\n",
       "    12   9357      0\n",
       "    10   8826  23499\n",
       "     9   1228      4\n",
       "    10      7    569\n",
       "     9      2    235\n",
       " 20059   2592   5909\n",
       "    90      3     20\n",
       "  3872    141      2\n",
       "    95      8   1450\n",
       "    49   6794    369\n",
       "     0   9046      5\n",
       "  3892   1497      2\n",
       "    24     13   2168\n",
       "   786      4    488\n",
       "    49     26   5967\n",
       " 28867     25    656\n",
       "     3  18430     14\n",
       "  6213     58     48\n",
       "     4   4886   4364\n",
       "  3872    217      4\n",
       "     5      5     22\n",
       "     2      2   1936\n",
       "  5050    593     59\n",
       "    95      7     14\n",
       "[torch.cuda.LongTensor of size 30x3 (GPU 0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.target[:, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful, the first dimension of the text and target is the sequence, and the next is the batch.\n",
    "We see that the target is indeed the original text offset by 1 (shifted downwards by 1). Which means we have all the we need to start training a language model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above iterators, training the language model is easy. \n",
    "\n",
    "First, we need to prepare the model. We'll be borrowing and customizing the model from the [examples](https://github.com/pytorch/examples/tree/master/word_language_model) in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable as V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp,\n",
    "                 nhid, nlayers, bsz,\n",
    "                 dropout=0.5, tie_weights=True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.nhid, self.nlayers, self.bsz = nhid, nlayers, bsz\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "        self.hidden = self.init_hidden(bsz) # the input is a batched consecutive corpus\n",
    "                                            # therefore, we retain the hidden state across batches\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, self.hidden = self.rnn(emb, self.hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (V(weight.new(self.nlayers, bsz, self.nhid).zero_().cuda()),\n",
    "                V(weight.new(self.nlayers, bsz, self.nhid).zero_()).cuda())\n",
    "    \n",
    "    def reset_history(self):\n",
    "        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "        self.hidden = tuple(V(v.data) for v in self.hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to explicitly pass the initial weights of the embedding matrix that are initialize with the GloVe vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(weight_matrix.size(0),\n",
    "                 weight_matrix.size(1), 200, 1, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.weight.data.copy_(weight_matrix);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin training the language model. We'll use the Adam optimizer here.\n",
    "\n",
    "For the loss, we'll use the `nn.CrossEntropyLoss` function. This loss takes the index of the correct class as the ground truth instead of a one-hot vector. Unfortunately, it only takes tensors of dimension 2 or 4, so we'll need to do a bit of reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = weight_matrix.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch):\n",
    "    \"\"\"One epoch of a training loop\"\"\"\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_iter):\n",
    "        # reset the hidden state or else the model will try to backpropagate to the\n",
    "        # beginning of the dataset, requiring lots of time and a lot of memory\n",
    "        model.reset_history()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, targets = batch.text, batch.target\n",
    "        prediction = model(text)\n",
    "        # pytorch currently only supports cross entropy loss for inputs of 2 or 4 dimensions.\n",
    "        # we therefore flatten the predictions out across the batch axis so that it becomes\n",
    "        # shape (batch_size * sequence_length, n_tokens)\n",
    "        # in accordance to this, we reshape the targets to be\n",
    "        # shape (batch_size * sequence_length)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.data[0] * prediction.size(0) * prediction.size(1)\n",
    "\n",
    "    epoch_loss /= len(train.examples[0].text)\n",
    "\n",
    "    # monitor the loss\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    for batch in valid_iter:\n",
    "        model.reset_history()\n",
    "        text, targets = batch.text, batch.target\n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        val_loss += loss.data[0] * text.size(0)\n",
    "    val_loss /= len(valid.examples[0].text)\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2217/2217 [01:59<00:00, 18.59it/s]\n",
      "/home/kurita/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:30: DeprecationWarning: generator 'BPTTIterator.__iter__' raised StopIteration\n",
      "  0%|          | 0/2217 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 6.2056, Validation Loss: 0.1711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2217/2217 [01:59<00:00, 18.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 5.2659, Validation Loss: 0.1599\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the output at 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(valid_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ids_to_sentence(id_tensor, vocab, join=None):\n",
    "    \"\"\"Converts a sequence of word ids to a sentence\"\"\"\n",
    "    if isinstance(id_tensor, torch.LongTensor):\n",
    "        ids = id_tensor.transpose(0, 1).contiguous().view(-1)\n",
    "    elif isinstance(id_tensor, np.ndarray):\n",
    "        ids = id_tensor.transpose().reshape(-1)\n",
    "\n",
    "    batch = [vocab.itos[ind] for ind in ids]  # denumericalize\n",
    "    if join is None:\n",
    "        return batch\n",
    "    else:\n",
    "        return join.join(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  <eos>   = homarus gammarus = <eos>   <eos>   homarus gammarus , known as the european lobster or common lobster , is a species of <unk> lobster from . <unk> ceo hiroshi <unk> referred to <unk> as one of his f'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids_to_sentence(b.text.cpu().data, TEXT.vocab, join=' ')[:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = model(b.text).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>   <eos> = = ( <eos>   <eos>   = = ( <unk> as the <unk> @-@ ( <unk> species , <unk> a <unk> of the <unk> ( the <eos> was <unk> <unk> <unk> to the the a of the first \" , the , <eos>   <eos> reviewers were t'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids_to_sentence(np.argmax(arrs, axis=2), TEXT.vocab, join=' ')[:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.. doesn't seem to be making much sense yet.\n",
    "Let's train for another 2 epochs and see how the results change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2217/2217 [01:59<00:00, 18.56it/s]\n",
      "/home/kurita/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:30: DeprecationWarning: generator 'BPTTIterator.__iter__' raised StopIteration\n",
      "  0%|          | 0/2217 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 4.9020, Validation Loss: 0.1568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2217/2217 [01:59<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 4.6959, Validation Loss: 0.1549\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs + 1, n_epochs * 2 + 1):\n",
    "    train_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>   <eos> = = ( <eos>   <eos>   <eos> ( ( is as the <unk> union <unk> <unk> starling <unk> <unk> the <unk> of the <unk> , the <eos> , <unk> <unk> , to the the a of the \" \" , the , <eos>   <eos> reviewers ha'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrs = model(b.text).cpu().data.numpy()\n",
    "word_ids_to_sentence(np.argmax(arrs, axis=2), TEXT.vocab, join=' ')[:210]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this getting better? The loss is certainly getting better.\n",
    "This just goes to show how difficult it is to match a loss value with the quality of the predictions in language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
